{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41e9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378b1475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x11eddabd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3266723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fada05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8dc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def llm(prompt):\n",
    "    response = ollama.chat(\n",
    "        model=\"gemma3:12b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def rag(query, debug=False):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    if debug:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7da92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "What is the capital of France?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 1: Docker and Terraform\n",
      "question: PGCLI - case sensitive use “Quotations” around columns with capital letters\n",
      "answer: PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: PGCLI - no pq wrapper available.\n",
      "answer: ImportError: no pq wrapper available.\n",
      "Attempts made:\n",
      "- couldn't import \\dt\n",
      "opg 'c' implementation: No module named 'psycopg_c'\n",
      "- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n",
      "- couldn't import psycopg 'python' implementation: libpq library not found\n",
      "Solution:\n",
      "First, make sure your Python is set to 3.9, at least.\n",
      "And the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n",
      "\n",
      "0. You can check your current python version with: \n",
      "$ python -V(the V must be capital)\n",
      "1. Based on the previous output, if you've got a 3.9, skip to Step #2\n",
      "   Otherwispye better off with a new environment with 3.9\n",
      "$ conda create –name de-zoomcamp python=3.9\n",
      "$ conda activate de-zoomcamp\n",
      "2. Next, you should be able to install the lib for postgres like this:\n",
      "```\n",
      "$ e\n",
      "$ pip install psycopg2_binary\n",
      "```\n",
      "3. Finally, make sure you're also installing pgcli, but use conda for that:\n",
      "```\n",
      "$ pgcli -h localhost -U root -d ny_taxisudo\n",
      "```\n",
      "There, you should be good to go now!\n",
      "Another solution:\n",
      "Run this\n",
      "pip install \"psycopg[binary,pool]\"\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - I am having problems with columns datatype while running DBT/BigQuery\n",
      "answer: Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\n",
      "✅Solution: Defined the schema while running web_to_gcp.py pipeline.\n",
      "Sebastian adapted the script:\n",
      "https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\n",
      "Need a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\n",
      "file_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\n",
      "open(file_name_gz, 'wb').write(r.content)\n",
      "os.system(f\"gzip -d {file_name_gz}\")\n",
      "os.system(f\"rm {file_name_init}.*\")\n",
      "Same ERROR - When running dbt run for fact_trips.sql, the task failed with error:\n",
      "“Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\n",
      "开启屏幕阅读器支持\n",
      "要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\n",
      "查找和替换\n",
      "Reason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\n",
      "There are some possible fixes:\n",
      "Drop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\n",
      "SELECT * EXCEPT (ehail_fee) FROM…\n",
      "Modify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\n",
      "Modify Airflow dag to make the conversion and avoid the error.\n",
      "pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\n",
      "Same type of ERROR - parquet files with different data types - Fix it with pandas\n",
      "Here is another possibility that could be interesting:\n",
      "You can specify the dtypes when importing the file from csv to a dataframe with pandas\n",
      "pd.from_csv(..., dtype=type_dict)\n",
      "One obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\n",
      "Sources:\n",
      "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
      "Nullable integer data type — pandas 1.5.3 documentation\n",
      "\n",
      "\n",
      "</CONTEXT>\n",
      "Answer: <generator object Client._request.<locals>.inner at 0x11e9eefc0>\n",
      "This document does not contain the capital of France."
     ]
    }
   ],
   "source": [
    "response = rag(\"What is the capital of France?\", debug=True)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.message.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e441c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This question cannot be answered from the provided context. The context does not contain information about how long it takes to learn data engineering."
     ]
    }
   ],
   "source": [
    "response = rag(\"is is possbile to learn data engineering in 6 months?\")\n",
    "\n",
    "for chunk in response:\n",
    "        print(chunk.message.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949f3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue: You're encountering a `ModuleNotFoundError: No module named 'psycopg2'` error.\n",
      "\n",
      "Solution: You can try installing `psycopg2-binary` using `pip install psycopg2-binary`. If that doesn's work, try updating it with `pip install psycopg2-binary --upgrade`. If the issue persists, try updating conda or pip before installing `psycopg2` again. If you're still facing errors related to `pg_config`, you may need to install PostgreSQL (e.g., `brew install postgresql` on macOS)."
     ]
    }
   ],
   "source": [
    "response = rag(\"No module named 'psycopg2'\")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.message.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ca9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
